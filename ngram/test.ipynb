{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0bc8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file\n",
    "with open(\"data/wiki.train.tokens\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "\n",
    "# replace unk with <unk>\n",
    "tokens = [word if word != \"unk\" else \"<unk>\" for word in tokens]\n",
    "\n",
    "# Remove rare words\n",
    "counter = Counter(tokens)\n",
    "rare_words = set(word for word, count in counter.items() if count < 5)\n",
    "tokens = [word if word not in rare_words else \"<unk>\" for word in tokens]\n",
    "\n",
    "counter = Counter(tokens)\n",
    "\n",
    "vocab = {word: i for i, (word, _) in enumerate(counter.items())}\n",
    "\n",
    "vocab[\"<unk>\"] = len(vocab)  # handle unknowns\n",
    "inv_vocab = {i: w for w, i in vocab.items()}\n",
    "vocab_size = len(vocab)+1  # +1 for <unk>\n",
    "# Encode tokens\n",
    "encoded = [vocab.get(word, vocab[\"<unk>\"]) for word in tokens]\n",
    "data = torch.tensor(encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f80b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    nbatch = len(data) // batch_size\n",
    "    data = data[:nbatch * batch_size]\n",
    "    return data.view(batch_size, -1).t().contiguous()\n",
    "\n",
    "batch_size = 32\n",
    "train_data = batchify(data, batch_size)\n",
    "\n",
    "# Get batch: source and target\n",
    "def get_batch(source, i, bptt=35):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a5c7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size, dropout=0.1, maxlen=5000):\n",
    "        super().__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pos_embedding[:x.size(0), :])\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, nhead, nhid, nlayers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.pos_encoder = PositionalEncoding(emb_size, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(emb_size, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.decoder = nn.Linear(emb_size, vocab_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.embedding(src) * math.sqrt(self.emb_size)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        return self.decoder(output)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd96e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = TransformerModel(\n",
    "    vocab_size=vocab_size,\n",
    "    emb_size=200,\n",
    "    nhead=2,\n",
    "    nhid=200,\n",
    "    nlayers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "def train(model, data, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.\n",
    "        for i in range(0, data.size(0) - 1, 35):\n",
    "            input_seq, target = get_batch(data, i)\n",
    "\n",
    "            src_mask = model.generate_square_subsequent_mask(input_seq.size(0)).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq.to(device), src_mask)\n",
    "            loss = loss_fn(output.view(-1, vocab_size), target.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss:.2f}\")\n",
    "\n",
    "def predict_next(model, text, vocab, inv_vocab, n_words=5):\n",
    "    model.eval()\n",
    "    tokens = text.lower().split()\n",
    "    input_ids = torch.tensor([vocab.get(w, vocab[\"<unk>\"]) for w in tokens], dtype=torch.long).unsqueeze(1).to(device)\n",
    "    mask = model.generate_square_subsequent_mask(input_ids.size(0)).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, mask)\n",
    "    last_logits = output[-1, 0, :]  # last timestep, first batch index\n",
    "    top_ids = torch.topk(torch.softmax(last_logits, dim=-1), k=n_words).indices\n",
    "    return [inv_vocab[i.item()] for i in top_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f02b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 9965.29\n",
      "Epoch 2 | Loss: 9252.32\n",
      "Epoch 3 | Loss: 8897.11\n",
      "Epoch 4 | Loss: 8644.11\n",
      "Epoch 5 | Loss: 8454.86\n",
      "Epoch 6 | Loss: 8308.26\n",
      "Epoch 7 | Loss: 8190.66\n",
      "Epoch 8 | Loss: 8094.43\n",
      "Epoch 9 | Loss: 8014.35\n",
      "Epoch 10 | Loss: 7941.40\n",
      "Epoch 11 | Loss: 7877.91\n",
      "Epoch 12 | Loss: 7823.14\n",
      "Epoch 13 | Loss: 7774.18\n",
      "Epoch 14 | Loss: 7727.42\n",
      "Epoch 15 | Loss: 7685.10\n",
      "Epoch 16 | Loss: 7646.91\n",
      "Epoch 17 | Loss: 7609.03\n",
      "Epoch 18 | Loss: 7574.76\n",
      "Epoch 19 | Loss: 7541.29\n",
      "Epoch 20 | Loss: 7510.80\n",
      "Epoch 21 | Loss: 7480.18\n",
      "Epoch 22 | Loss: 7451.75\n",
      "Epoch 23 | Loss: 7423.63\n",
      "Epoch 24 | Loss: 7397.26\n",
      "Epoch 25 | Loss: 7372.31\n",
      "Epoch 26 | Loss: 7348.31\n",
      "Epoch 27 | Loss: 7323.03\n",
      "Epoch 28 | Loss: 7299.12\n",
      "Epoch 29 | Loss: 7277.05\n",
      "Epoch 30 | Loss: 7254.97\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60a2ef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weeks', 'countries', 'years', 'days', 'other']\n"
     ]
    }
   ],
   "source": [
    "print(predict_next(model, \"in several \", vocab, inv_vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
